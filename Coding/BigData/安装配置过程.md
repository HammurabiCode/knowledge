# 分布式安装
 1. 安装Ubuntu
  - 如果使用虚拟机，网卡要桥接，并使用不一样的Mac地址
 2. 安装配置 SSH
  - ssh是使用scp的基础
  - 实现无密码登陆
   1. ssh-keygen -t rsa 产生公钥和私钥
   2. cat id_rsa.pub >> ~/.ssh/authorized_keys 将公钥放入~/.ssh/authorized_keys
   3.

  3. 安装Java环境
   - apt-get install default-jdk
   - 配置JAVA_HOME: dpkg -L openjdk-8-jdk | grep 'bin/javac'

  4. 安装Hadoop
   - 下载
   - 解压 tar -xzvf
   - 拷贝到 /usr/local/
   - 修改用户 chown

  5. 配置Hadoop
   - 关闭Hadoop再配置：/usr/local/hadoop/sbin/stop-dfs.sh
   - 网络配置：虚拟机需要选择桥接网卡并适用不一样的MAC地址
    * 修改主机名：/etc/hostname
    * 修改IP映射：/etc/host中加入
	     192.168.56.2 Master
	     192.168.56.3 Slave1 // ubuntu desktop
	     192.168.56.4 Slave2 // ubuntu server
   - SSH无密码登陆
    * 生成公钥私钥
    * 公钥加入到~/authorized_keys当中
    * 拷贝给其他节点Slave1：scp ~/.ssh/id_rsa.pub hadoop@Slave1:/home/hadoop/
    * 加入到Slave1的authorized_keys中
   - 配置PATH变量（只在master上配置即可） ~/.bashrc中
    export PATH=$PATH:/usr/local/Hadoop/bin:/usr/local/hadoop/sbin

 6. 配置集群分布式环境
  - 需要修改5个文件/usr/local/hadoop/etc/hadoop/
   * slaves: 每个DataNode一行
   * core-site.xml
   	- fs.defaultFS -> hdfs://Master:9000
   	- hadoop.tmp.dir -> /usr/local/hadoop/tmp
   * hdfs-site.xml
    - dfs.replication -> 2
    - dfs.namenode.secondary.http-address -> Master:50090
    - dfs.namenode.name.dir -> file:/usr/local/hadoop/tmp/dfs/name
    - dfs.datanode.data.dir -> file:/usr/local/hadoop/tmp/dfs/data
   * mapred-site.xml
   * yarn-site.xml 为了在其他电脑上访问，需要配置yarn的一些参数
    - 


